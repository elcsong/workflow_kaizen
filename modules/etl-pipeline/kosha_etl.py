import requests
import pandas as pd
import json
import os
import time
import argparse
import platform
from pathlib import Path
import xml.etree.ElementTree as ET

# Selenium for robust data extraction - cross-platform support
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# Cross-platform WebDriver imports
if platform.system() == "Windows":
    from selenium.webdriver.edge.options import Options as EdgeOptions
    from selenium.webdriver.edge.service import Service as EdgeService
else:
    # For Mac/Linux development, use Chrome as fallback
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from selenium.webdriver.chrome.service import Service as ChromeService

# WebDriver manager for automatic driver installation
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.microsoft import EdgeChromiumDriverManager

# KOSHA (Korea Occupational Safety and Health Agency) data sources
# 산업안전보건법 특수관리물질 관련 데이터 소스 설정
KOSHA_CONFIG = {
    'special_materials': {
        'name': '특수관리물질',
        'description': '산업안전보건법 제41조에 따른 특수관리물질 목록',
        'base_url': 'https://www.kosha.or.kr/kosha/index.do',  # 기본 KOSHA URL
        'data_url': 'https://www.kosha.or.kr/kosha/data/list.do',  # 데이터 목록 페이지
        'api_base': 'https://www.kosha.or.kr/kosha/api',  # API 기본 URL (있는 경우)
        'search_keywords': ['특수관리물질', 'special management substances', 'dangerous substances', '관리물질'],
        'expected_columns': [
            '물질명', '영문명', 'CAS_No', '관리기준', '관리방법', '비고',
            'substance_name', 'cas_number', 'management_criteria', 'management_method'
        ],
        'known_data_urls': [
            'https://www.kosha.or.kr/kosha/data/list.do',  # 데이터 목록
            'https://www.kosha.or.kr/kosha/chemical/list.do',  # 화학물질 목록
            'https://www.kosha.or.kr/kosha/law/list.do'  # 법규 목록
        ]
    },
    'hazardous_materials': {
        'name': '유해화학물질',
        'description': '화학물질안전원에서 관리하는 유해화학물질 목록',
        'base_url': 'https://www.nics.go.kr/',  # 화학물질안전원
        'data_url': 'https://www.nics.go.kr/chem/',  # 화학물질 데이터
        'api_base': 'https://www.nics.go.kr/api',
        'search_keywords': ['유해화학물질', 'hazardous chemicals', 'toxic substances', '화학물질'],
        'expected_columns': [
            '물질명', '영문명', 'CAS_No', '유해성', '관리등급', '비고'
        ],
        'known_data_urls': [
            'https://www.nics.go.kr/chem/list.do',
            'https://www.nics.go.kr/substance/list.do'
        ]
    },
    # 추가: 고용노동부 데이터
    'mole_data': {
        'name': '고용노동부 화학물질 데이터',
        'description': '고용노동부에서 제공하는 화학물질 안전 데이터',
        'base_url': 'https://www.moel.go.kr/',
        'data_url': 'https://www.moel.go.kr/policy/policydata/view.do',
        'api_base': 'https://www.moel.go.kr/api',
        'search_keywords': ['화학물질', '안전보건', 'chemical', 'safety'],
        'expected_columns': [
            '물질명', 'CAS번호', '관리등급', '유해성정보'
        ],
        'known_data_urls': [
            'https://www.moel.go.kr/policy/policydata/view.do',
            'https://www.moel.go.kr/info/hygine/bbs/list.do'
        ]
    }
}

def _default_headers(base_url: str) -> dict:
    """Return browser-like headers for Korean government website requests."""
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': base_url,
        'Connection': 'keep-alive',
    }

def _build_webdriver(download_dir: Path):
    """Create cross-platform WebDriver in headless mode with download directory configured."""
    system = platform.system()

    # Browser priority: Chrome first (cross-platform), then Edge on Windows
    browser_configs = []

    if system == "Windows":
        browser_configs = [
            (ChromeOptions, webdriver.Chrome, "Chrome"),
            (EdgeOptions, webdriver.Edge, "Edge")
        ]
    else:
        # Mac/Linux: Chrome primarily
        browser_configs = [
            (ChromeOptions, webdriver.Chrome, "Chrome")
        ]

    # Common options for all browsers
    def setup_options(opts):
        opts.add_argument('--headless=new')
        opts.add_argument('--disable-gpu')
        opts.add_argument('--no-sandbox')
        opts.add_argument('--disable-dev-shm-usage')
        opts.add_argument('--disable-web-security')
        opts.add_argument('--allow-running-insecure-content')
        opts.add_argument('--lang=ko-KR')

        # Ensure downloads go to the specified folder without prompts
        download_path = str(download_dir.resolve())
        prefs = {
            'download.default_directory': download_path,
            'download.prompt_for_download': False,
            'download.directory_upgrade': True,
            'safebrowsing.enabled': True,
        }
        opts.add_experimental_option('prefs', prefs)
        return opts, download_path

    # Try each browser configuration
    last_error = None
    for opts_class, driver_class, browser_name in browser_configs:
        try:
            opts, download_path = setup_options(opts_class())

            # Set up service with webdriver-manager
            if browser_name == "Chrome":
                service = ChromeService(ChromeDriverManager().install())
            elif browser_name == "Edge":
                service = EdgeService(EdgeChromiumDriverManager().install())
            else:
                service = None

            # Initialize driver with service if available
            if service:
                driver = driver_class(service=service, options=opts)
            else:
                driver = driver_class(options=opts)

            # Force download behavior via CDP (Chromium-based browsers)
            driver.execute_cdp_cmd('Page.setDownloadBehavior', {
                'behavior': 'allow',
                'downloadPath': download_path
            })
            print(f"Successfully initialized {browser_name} WebDriver")
            return driver
        except Exception as e:
            last_error = e
            print(f"{browser_name} WebDriver initialization failed: {e}")
            continue

    # All browsers failed
    error_msg = f"All WebDriver initializations failed. Last error: {last_error}"
    if system == "Windows":
        error_msg += "\nMake sure Chrome or Edge WebDriver is installed. Try: pip install webdriver-manager"
    else:
        error_msg += "\nMake sure Chrome WebDriver is installed. Try: pip install webdriver-manager"
    raise Exception(error_msg)

def _wait_for_new_file(download_dir: Path, file_pattern: str, before_files: set[str], timeout: int = 120) -> Path:
    """Wait for a new file to appear and finish downloading in download_dir."""
    end_time = time.time() + timeout
    while time.time() < end_time:
        current_files = {p.name for p in download_dir.glob(file_pattern)}
        new_files = current_files - before_files
        # If file visible and no partial download files present
        partial_patterns = ['*.crdownload', '*.part', '*.tmp']
        has_partial = any(any(download_dir.glob(pattern)) for pattern in partial_patterns)

        if new_files and not has_partial:
            # Choose the most recently modified new file
            candidates = sorted(
                [download_dir / name for name in new_files],
                key=lambda p: p.stat().st_mtime,
                reverse=True,
            )
            return candidates[0]
        time.sleep(0.5)
    raise TimeoutError(f'{file_pattern} download did not complete within timeout')

def _handle_cookie_consent(driver):
    """Handle cookie consent banners on Korean government sites."""
    try:
        cookie_buttons = [
            (By.CSS_SELECTOR, 'button[id*="cookie"]'),
            (By.CSS_SELECTOR, 'button[class*="cookie"]'),
            (By.XPATH, "//button[contains(text(), '동의') or contains(text(), '수락') or contains(text(), 'Accept') or contains(text(), '同意')]"),
            (By.CSS_SELECTOR, 'button#onetrust-accept-btn-handler'),
            (By.CSS_SELECTOR, 'button[data-testid*="cookie-accept"]'),
        ]
        for by, sel in cookie_buttons:
            try:
                el = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((by, sel)))
                el.click()
                print("Accepted cookie consent")
                time.sleep(1)
                break
            except TimeoutException:
                continue
    except Exception:
        pass

def search_kosha_data(data_type: str) -> dict:
    """Search for KOSHA data using web scraping."""
    config = KOSHA_CONFIG.get(data_type)
    if not config:
        raise ValueError(f"Unknown data type: {data_type}")

    download_dir = Path('data')
    download_dir.mkdir(parents=True, exist_ok=True)

    driver = _build_webdriver(download_dir)
    try:
        # First try known data URLs
        all_search_results = []

        known_urls = config.get('known_data_urls', [])
        for url in known_urls:
            try:
                driver.get(url)
                wait = WebDriverWait(driver, 30)

                # Handle cookie banners
                _handle_cookie_consent(driver)

                # Search for data download links
                download_links = driver.find_elements(By.XPATH, "//a[contains(@href, '.xlsx') or contains(@href, '.xls') or contains(@href, '.csv') or contains(@href, '.pdf') or contains(@href, 'download') or contains(@href, 'excel')]")
                for link in download_links:
                    href = link.get_attribute('href')
                    text = link.get_text().strip()
                    if href and any(keyword in text.lower() for keyword in config['search_keywords']):
                        all_search_results.append({
                            'url': href,
                            'title': text,
                            'source_url': url,
                            'type': 'download_link'
                        })

                # Look for data tables on the page
                tables = driver.find_elements(By.TAG_NAME, 'table')
                for table in tables:
                    headers = []
                    header_elements = table.find_elements(By.TAG_NAME, 'th')
                    for header in header_elements:
                        headers.append(header.text.strip())

                    if any(col in ' '.join(headers) for col in config['expected_columns']):
                        all_search_results.append({
                            'table_headers': headers,
                            'source_url': url,
                            'type': 'data_table'
                        })

            except Exception as e:
                print(f"Failed to search URL {url}: {e}")
                continue

        # Also try the main data_url
        try:
            driver.get(config['data_url'])
            _handle_cookie_consent(driver)

            # Search for data download links
            download_links = driver.find_elements(By.XPATH, "//a[contains(@href, '.xlsx') or contains(@href, '.xls') or contains(@href, '.csv') or contains(@href, '.pdf') or contains(@href, 'download') or contains(@href, 'excel')]")
            for link in download_links:
                href = link.get_attribute('href')
                text = link.get_text().strip()
                if href and any(keyword in text.lower() for keyword in config['search_keywords']):
                    all_search_results.append({
                        'url': href,
                        'title': text,
                        'source_url': config['data_url'],
                        'type': 'download_link'
                    })

        except Exception as e:
            print(f"Failed to search main data URL {config['data_url']}: {e}")

        return {
            'data_type': data_type,
            'config': config,
            'search_results': all_search_results,
            'page_title': driver.title if hasattr(driver, 'title') else 'N/A'
        }

    finally:
        driver.quit()

def extract_table_data(driver, table_selector: str = None) -> pd.DataFrame:
    """Extract data from HTML table."""
    if table_selector:
        table = driver.find_element(By.CSS_SELECTOR, table_selector)
    else:
        table = driver.find_element(By.TAG_NAME, 'table')

    # Extract headers
    headers = []
    header_row = table.find_element(By.TAG_NAME, 'thead')
    if header_row:
        header_cells = header_row.find_elements(By.TAG_NAME, 'th')
        headers = [cell.text.strip() for cell in header_cells]

    # Extract data rows
    rows = []
    tbody = table.find_element(By.TAG_NAME, 'tbody')
    data_rows = tbody.find_elements(By.TAG_NAME, 'tr')

    for row in data_rows:
        cells = row.find_elements(By.TAG_NAME, 'td')
        row_data = [cell.text.strip() for cell in cells]
        if row_data:
            rows.append(row_data)

    # Create DataFrame
    if headers and rows:
        df = pd.DataFrame(rows, columns=headers)
    else:
        df = pd.DataFrame(rows)

    return df

def try_api_extraction(data_type: str) -> dict:
    """Try to extract data using API endpoints."""
    config = KOSHA_CONFIG.get(data_type)
    api_base = config.get('api_base')

    if not api_base:
        return None

    session = requests.Session()
    base_headers = _default_headers(config['base_url'])

    # Common Korean government API patterns
    api_endpoints = [
        f"{api_base}/list",  # 목록 조회
        f"{api_base}/data",  # 데이터 조회
        f"{api_base}/chemicals",  # 화학물질 데이터
        f"{api_base}/substances",  # 물질 데이터
    ]

    for endpoint in api_endpoints:
        try:
            response = session.get(endpoint, headers=base_headers, timeout=30)
            if response.status_code == 200:
                content_type = response.headers.get('Content-Type', '').lower()

                if 'json' in content_type:
                    data = response.json()
                    return {
                        'source': 'api',
                        'endpoint': endpoint,
                        'data': data,
                        'content_type': content_type
                    }
                elif 'xml' in content_type:
                    # Parse XML
                    root = ET.fromstring(response.content)
                    return {
                        'source': 'api',
                        'endpoint': endpoint,
                        'data': root,
                        'content_type': content_type
                    }

        except Exception as e:
            print(f"API endpoint {endpoint} failed: {e}")
            continue

    return None

def download_excel_from_link(driver, link_url: str, download_dir: Path) -> str:
    """Download Excel file from a link using Selenium."""
    before_files = {p.name for p in download_dir.glob('*.xls*')}

    driver.get(link_url)
    time.sleep(2)  # Wait for download to start

    downloaded_path = _wait_for_new_file(download_dir, '*.xls*', before_files, timeout=180)
    print(f"Downloaded Excel file: {downloaded_path}")
    return str(downloaded_path)

def _read_excel_robust(path: str) -> pd.DataFrame:
    """Read Excel file with multiple encoding attempts."""
    encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']  # Korean encodings

    for encoding in encodings:
        try:
            df = pd.read_excel(path, engine='openpyxl')
            if not df.empty:
                print(f"Successfully read Excel with encoding: {encoding}")
                return df
        except Exception as e:
            print(f"Failed to read Excel with encoding {encoding}: {e}")
            continue

    # Try with different engines
    try:
        df = pd.read_excel(path, engine='xlrd')
        print("Successfully read Excel with xlrd engine")
        return df
    except Exception as e:
        print(f"Failed to read Excel with xlrd: {e}")

    raise ValueError(f"Could not read Excel file: {path}")


def etl_process_kosha(data_type: str, skip_download: bool = False) -> dict:
    """ETL process for KOSHA data."""
    import logging

    # 로깅 설정
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    config = KOSHA_CONFIG.get(data_type)
    logger.info(f"Starting ETL process for {data_type}")

    if not config:
        error_msg = f"Unknown data type: {data_type}"
        logger.error(error_msg)
        raise ValueError(error_msg)

    # Try API first
    logger.info("Attempting API data extraction")
    api_data = try_api_extraction(data_type)
    if api_data:
        logger.info(f"Successfully extracted data from API: {api_data['endpoint']}")
        # Process API data (JSON or XML)
        if isinstance(api_data['data'], dict):
            data = api_data['data']
        elif hasattr(api_data['data'], 'findall'):  # XML
            # Parse XML similar to REACH
            root = api_data['data']
            data = []
            for row in root.findall('.//item'):  # Common XML pattern
                row_dict = {}
                for col in row:
                    key = col.tag.lower().replace(' ', '_')
                    row_dict[key] = col.text.strip() if col.text else None
                data.append(row_dict)
        else:
            data = api_data['data']

        metadata = {
            'data_type': data_type,
            'source': 'api',
            'item_count': len(data) if isinstance(data, list) else 'N/A',
            'api_endpoint': api_data['endpoint']
        }
        logger.info(f"API extraction completed. Items: {metadata['item_count']}")
        return {'metadata': metadata, 'data': data}
    else:
        logger.warning("API extraction failed, no data returned")

    # Fallback to web scraping
    logger.info("API extraction failed, attempting web scraping")
    try:
        search_results = search_kosha_data(data_type)

        data_found = False
        processed_data = []

        download_dir = Path('data')
        driver = _build_webdriver(download_dir)
        try:
            logger.info(f"Processing {len(search_results['search_results'])} search results")

            for result in search_results['search_results']:
                if result['type'] == 'download_link':
                    try:
                        logger.info(f"Attempting to download Excel from: {result['url']}")
                        excel_path = download_excel_from_link(driver, result['url'], download_dir)
                        df = _read_excel_robust(excel_path)
                        processed_data.extend(df.to_dict('records'))
                        data_found = True
                        logger.info(f"Successfully extracted {len(df)} records from Excel file")
                    except Exception as e:
                        logger.error(f"Failed to download/process Excel: {e}")
                        continue

                elif result['type'] == 'data_table':
                    try:
                        source_url = result.get('source_url', search_results['config']['data_url'])
                        logger.info(f"Attempting to extract table data from: {source_url}")
                        driver.get(source_url)
                        df = extract_table_data(driver)
                        if not df.empty:
                            processed_data.extend(df.to_dict('records'))
                            data_found = True
                            logger.info(f"Successfully extracted {len(df)} records from HTML table")
                    except Exception as e:
                        logger.error(f"Failed to extract table data: {e}")
                        continue

            if not data_found:
                error_msg = f"No data found for {data_type} from web scraping"
                logger.error(error_msg)
                raise ValueError(error_msg)

            metadata = {
                'data_type': data_type,
                'source': 'web_scraping',
                'item_count': len(processed_data),
                'config': config
            }

            logger.info(f"Web scraping completed successfully. Total items: {len(processed_data)}")
            return {'metadata': metadata, 'data': processed_data}

        finally:
            driver.quit()

    except Exception as e:
        logger.error(f"Web scraping failed: {e}")
        raise

def main():
    parser = argparse.ArgumentParser(description='KOSHA ETL Pipeline for Korean Chemical Safety Data')
    parser.add_argument('--data-type', choices=['special_materials', 'hazardous_materials', 'mole_data'],
                       default='special_materials', help='Type of data to extract')
    parser.add_argument('--skip-download', action='store_true', help='Skip downloading and use existing files')
    parser.add_argument('--output-file', default='kosha_data.json', help='Output JSON file name')
    args = parser.parse_args()

    os.makedirs('data', exist_ok=True)

    try:
        result = etl_process_kosha(args.data_type, skip_download=args.skip_download)

        output_file = f'data/{args.output_file}'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=4, ensure_ascii=False)
        print(f"Data saved to {output_file}")
        print(f"Extracted {result['metadata']['item_count']} items")

    except Exception as e:
        print(f"ETL process failed: {e}")
        raise

if __name__ == "__main__":
    main()
